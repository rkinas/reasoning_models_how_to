# Thinking model research notes

# Datasets
- [Y] https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- 
- [Y] https://huggingface.co/datasets/simplescaling/s1K-1.1
- [Y] https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k
- [Y] https://huggingface.co/datasets/GAIR/LIMO
- [Y] https://huggingface.co/datasets/AI-MO/NuminaMath-CoT

LIMR: Less is More for RL Scaling - only math question for RL training - This repository presents LIMR, an approach that challenges the assumption about data scaling in reinforcement learning for LLMs. We demonstrate that the quality and relevance of training samples matter far more than their quantity. Our Learning Impact Measurement (LIM) methodology enables automated evaluation of training sample effectiveness, eliminating the need for manual curation while achieving comparable or superior results with 6x less data. Notably, all our investigations are conducted directly from base models without distillation, providing clear insights into the core dynamics of RL training.
- https://huggingface.co/datasets/GAIR/LIMR

# Evaluation 
- https://github.com/huggingface/open-r1
  
